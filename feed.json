{
    "version": "https://jsonfeed.org/version/1",
    "title": "Memoria",
    "description": "",
    "home_page_url": "https://freshflash4096.github.io",
    "feed_url": "https://freshflash4096.github.io/feed.json",
    "user_comment": "",
    "author": {
        "name": "Huie"
    },
    "items": [
        {
            "id": "https://freshflash4096.github.io/the-product-behind-the-benchmarks-905p-960gb/",
            "url": "https://freshflash4096.github.io/the-product-behind-the-benchmarks-905p-960gb/",
            "title": "못 다한 이야기 - 905P 960GB",
            "summary": "본격적인 제 SSD 리뷰의 첫 글로 벤치마크 방법에 대해 작성했습니다. 하지만 이는&hellip;",
            "content_html": "<p>본격적인 제 SSD 리뷰의 <a href=\"https://freshflash4096.github.io/benchmark-method/\">첫 글</a>로 벤치마크 방법에 대해 작성했습니다.</p>\n<p>하지만 이는 온전히 성능 벤치마크에 관한 글이었고, 이 때문에 제가 생각하던 리뷰의 원래 양식에는 맞추지 못했는데, 이 글을 통해 905P에 대한 내용을 약간 추가하고자 합니다.</p>\n<p>별거 없지만 시작해보죠.</p>\n<div class=\"post__toc\">\n<h3>목차</h3>\n<ul>\n<li><a href=\"#mcetoc_1iup18slp5s\">Appearance</a></li>\n<li><a href=\"#mcetoc_1iup1bg6i63\">Internal Components</a>\n<ul>\n<li><a href=\"#mcetoc_1iup3oiobb2\">Notable Points</a>\n<ul>\n<li><a href=\"#mcetoc_1iup50imcdg\">Over Provisioning on 3D XPoint?</a></li>\n<li><a href=\"#mcetoc_1iup4vgr6dc\">Performance Gudie</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"#mcetoc_1iup47dknb5\">Datasheet</a></li>\n<li><a href=\"#mcetoc_1iup4vgr6dd\">SW Report</a></li>\n<li><a href=\"#mcetoc_1iup4vgr6dd\">DUT Summary</a></li>\n<li><a href=\"#mcetoc_1iuqh3uem5o\">Closing</a></li>\n</ul>\n</div>\n<hr>\n<h2 id=\"mcetoc_1iup18slp5s\">Appearance</h2>\n<div class=\"gallery-wrapper\"><div class=\"gallery\"  data-is-empty=\"false\" data-translation=\"이미지 추가\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/32/gallery/905P_Box_1.jpg\" data-size=\"4128x2580\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/32/gallery/905P_Box_1-thumbnail.jpg\" alt=\"\" width=\"768\" height=\"480\"></a>\n<figcaption>Outside Box</figcaption>\n</figure>\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/32/gallery/905P_Box_2.jpg\" data-size=\"4128x2580\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/32/gallery/905P_Box_2-thumbnail.jpg\" alt=\"\" width=\"768\" height=\"480\"></a>\n<figcaption>Inside Box</figcaption>\n</figure>\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/32/gallery/905P_Box_3.jpg\" data-size=\"4128x2580\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/32/gallery/905P_Box_3-thumbnail.jpg\" alt=\"\" width=\"768\" height=\"480\"></a>\n<figcaption>Product in Box</figcaption>\n</figure>\n</div></div>\n<p>제가 가진 것 중에서 유일하게 박스가 온전히 있는 SSD입니다. 박스를 열면 조심스레 포장된 SSD가 반겨줍니다. 기본으로 제공되는 LP 브라켓과 함께 말이죠.</p>\n<p>2018년 2분기에 출시되었고, 당시 가격이 $1,602라는 어마어마한 수치였네요. 단종된 지금도 동용량, 동세대 제품에 비해 중고시세가 더 높게 책정이 되어있습니다.</p>\n<div class=\"gallery-wrapper\"><div class=\"gallery\"  data-is-empty=\"false\" data-translation=\"이미지 추가\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/32/gallery/905P_Product_Front.jpg\" data-size=\"4128x2580\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/32/gallery/905P_Product_Front-thumbnail.jpg\" alt=\"\" width=\"768\" height=\"480\"></a>\n<figcaption>905P Front</figcaption>\n</figure>\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/32/gallery/905P_Product_Rear.jpg\" data-size=\"4128x2580\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/32/gallery/905P_Product_Rear-thumbnail.jpg\" alt=\"\" width=\"768\" height=\"480\"></a>\n<figcaption>905P Rear</figcaption>\n</figure>\n</div></div>\n<p>본품 사진입니다. PCIe 슬롯에 직접 장착하는 AIC타입이며, 폼팩터는 HHHL입니다. 방열판과 백플레이트로 꼼꼼히 무장된 것을 확인할 수 있는데, AIC타입의 P4800X에 백플레이트가 없는 것과는 대조적입니다.</p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://freshflash4096.github.io/media/posts/32/905P_Teardown_Heatsink.jpg\" alt=\"\" width=\"4128\" height=\"2580\"><figcaption>905P Heatsink &amp; Backplate</figcaption></figure>\n<p>방열판엔 SW로 Blue-Red-Green 색상 조절이 가능한 LED가\b달려있으며, 방열판의 아랫면에서 관련 접점을 확인할 수 있습니다.</p>\n<p>그럼, 조금 더 자세하게 들어가보겠습니다. </p>\n<hr>\n<h2 id=\"mcetoc_1iup1bg6i63\">Internal Components</h2>\n<div class=\"gallery-wrapper\"><div class=\"gallery\"  data-is-empty=\"false\" data-translation=\"이미지 추가\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/32/gallery/905P_Teardown_Front-2.jpg\" data-size=\"4128x2580\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/32/gallery/905P_Teardown_Front-2-thumbnail.jpg\" alt=\"\" width=\"768\" height=\"480\"></a>\n<figcaption>905P Teardown Front</figcaption>\n</figure>\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/32/gallery/905P_Teardown_Rear.jpg\" data-size=\"4128x2580\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/32/gallery/905P_Teardown_Rear-thumbnail.jpg\" alt=\"\" width=\"768\" height=\"480\"></a>\n<figcaption>905P Teardown Rear</figcaption>\n</figure>\n</div></div>\n<pre>문외한의 입장에서 일부 IC만 찾아보았으며, 제조시기 등에 따라 변경될 가능성이 있습니다.</pre>\n<table style=\"border-collapse: collapse; width: 100%; height: 303.625px;\" border=\"1\">\n<tbody>\n<tr style=\"height: 48.5625px;\">\n<td style=\"width: 49.9287%; background-color: #34495e; height: 48.5625px;\"><span style=\"color: #ffffff;\">Intel Optane SSD 905P (960GB, HHHL)</span></td>\n<td style=\"width: 50.0713%; background-color: #34495e; height: 48.5625px;\"><span style=\"color: #ffffff;\">Package Date 2018. 04. 03</span></td>\n</tr>\n<tr style=\"height: 51.625px;\">\n<td style=\"width: 49.9287%; height: 51.625px;\"><strong><span style=\"color: #e03e2d;\">Intel SLL3D</span></strong></td>\n<td style=\"width: 50.0713%; height: 51.625px;\"><span style=\"color: #3598db;\">Texas Instruments TPS53513</span></td>\n</tr>\n<tr style=\"height: 51.625px;\">\n<td style=\"width: 49.9287%; height: 51.625px;\"><strong><span style=\"color: #e67e23;\">Intel 29P32B1AMDNF2 *(7+14)</span></strong></td>\n<td style=\"width: 50.0713%; height: 51.625px;\"><span style=\"color: #c2e0f4;\">Texas Instruments TPS62130</span></td>\n</tr>\n<tr style=\"height: 51.625px;\">\n<td style=\"width: 49.9287%; height: 51.625px;\"><strong><span style=\"color: #f1c40f;\">Intel 29P64B1ANDNF3 *7</span></strong></td>\n<td style=\"width: 50.0713%; height: 51.625px;\"><span style=\"color: #ff00fc;\">Texas Instruments TPS62150</span></td>\n</tr>\n<tr style=\"height: 51.625px;\">\n<td style=\"width: 49.9287%; height: 51.625px;\"><span style=\"color: #2dc26b;\">IDT P78001-50</span></td>\n<td style=\"width: 50.0713%; height: 51.625px;\"><span style=\"color: #755127;\">Texas Instruments TPS54240</span></td>\n</tr>\n<tr style=\"height: 48.5625px;\">\n<td style=\"width: 49.9287%; height: 48.5625px;\">.</td>\n<td style=\"width: 50.0713%; height: 48.5625px;\"><span style=\"color: #b96ad9;\">Microchip PIC16F1829</span></td>\n</tr>\n</tbody>\n</table>\n<p>컨트롤러는 <span style=\"color: #e03e2d;\">Intel SLL3D</span>를 사용합니다. Optane SSD 제품 중에서 900P, 905P, P4800X에 적용되었으며, ARM Cortex-R5 아키텍쳐에 7CH로 알려져있습니다.</p>\n<p>일반적인 NAND Flash가 아닌 3D XPoint로 홍보하는 PCM이 저장매체로 사용됩니다. 905P 960GB에서 사용된 파트는 <span style=\"color: #e67e23;\">Intel 29P32BIAMDNF2</span>과 <span style=\"color: #f1c40f;\">Intel 29P64B1ANDNF3</span>입니다. 전자는 128Gb Die에 DDP로 32GB 용량을, 후자는 동일한 Die에 QDP로 64GB 용량을 구현합니다. 총합의 원시용량은 1120GB입니다.</p>\n<p>PMIC는 Intel SSD에서 자주보이는 <span style=\"color: #2dc26b;\">IDT P78001-50</span>이 채용되었습니다. PN에서 50이 뜻하는 바는 알 수 없지만, P78001 자체는 꽤 자주 보이는 친구로, 제가 보유했던 SSD 기준으로도 몇 번씩 확인할 수 있던 PN입니다. IDT사에서 Intel 맞춤형으로 만든 PMIC로 알려져있습니다.</p>\n<p>그외에 <span style=\"color: #b96ad9;\">Microchip PIC16F1829</span>이라는 MCU도 볼 수 있었습니다.</p>\n<hr>\n<h3 id=\"mcetoc_1iup3oiobb2\">Notable Points</h3>\n<h4 id=\"mcetoc_1iup50imcdg\">Over Provisioning on 3D XPoint?</h4>\n<figure class=\"post__image\" ><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/32/seukeurinsyas-2025-06-28-ojeon-12.57.02.png\" alt=\"\" width=\"2210\" height=\"1482\">\n<figcaption >\bSource: <a href=\"https://community.intel.com/t5/Intel-Optane-Memory/Intel-800P-900P-and-over-provisioning/m-p/544089\">Intel Community</a></figcaption>\n</figure>\n<p>3D XPoint는 NAND와 다르게 엄격한 Over-Provisioning이 필요없다고 알려져 있습니다. 이는 DRAM과 같은 byte-addressable이라는 특징 덕분에 write-in-place가 가능하기 때문입니다. 그럼에도 불구하고 실제로는 17%정도 여유가 있는 것을 확인할 수 있었는데, 가볍게 생각해보면 아래와 같은 가능성을 제시할 수 있습니다.</p>\n<ul>\n<li>예비 저장공간\n<ul>\n<li>몇몇 사람들이 알고있는 것과 다르게 3D XPoint엔 쓰기 수명이 존재합니다. 이에 대비한 예비 영역일 가능성이 제시됩니다.</li>\n</ul>\n</li>\n<li>ECC\n<ul>\n<li>3D XPoint Die에 ECC영역이 없을 가능성이 있습니다. </li>\n</ul>\n</li>\n</ul>\n<p>실제 사용 목적은 마지막까지 공개하지 않은 것으로 알고 있습니다. </p>\n<h4 id=\"mcetoc_1iup4vgr6dc\">Performance Gudie</h4>\n<figure class=\"post__image\" ><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/32/seukeurinsyas-2025-06-28-ohu-2.45.30.png\" alt=\"\" width=\"1130\" height=\"448\">\n<figcaption >\bSource: Intel Optane Solid State Drives for Client - Evaluation Guide</figcaption>\n</figure>\n<p>벤치마크 방법을 소개하는 글에서 NAND기반 저장장치는 성능 변화가 발생하기에 Pre-Conditioning이 요구된다고 말했습니다. 3D XPoint는 NAND와 달리 이런 제약에서 자유롭지만 Evaluation Guide를 살펴보면 약간 특이한 사항이 있습니다.</p>\n<p>바로 성능 측정 전에 최소 3시간 동안 전원을 켜두라는 점입니다.</p>\n<p>이는 Background Data Refresh작업 때문인데요, NAND와 마찬가지로 3D XPoint 매체 또한 주기적인 Refresh가 필요합니다. 이러한 작업은 성능에 영향을 줄 수 있기 때문에 설계 기준으로 Refresh가 수행되는 3시간 동안 전원을 켜두라고 요구합니다.</p>\n<p>다만, 이는 1세대 3D XPoint 매체가 적용된 SSD, 900P 또는 P4800X에 대한 가이드이기에 2세대가 적용된 SSD는 다를 수도 있습니다. 이에 대해서는 언젠가 직접 실험해보도록 하겠습니다.</p>\n<hr>\n<h2 id=\"mcetoc_1iup47dknb5\">Datasheet</h2>\n<p>소제목을 \"Datasheet\"라고 웅장하게 지은 것과 다르게, Optane SSD는 단 하나도 데이터시트가 공개되지 않았습니다. 기본적으로 Intel SSD를 종종 찾아보면 Intel Confidential이라고 적혀있는 문서가 가끔씩 널려있는게 보이는데... Optane SSD만큼은 예외입니다. </p>\n<p>몇 년 동안 찾아보려 노력했지만, Product Brief 이외엔 단 하나도 찾아볼 수 없었습니다. 엄격하게 관리되고 있단 뜻이겠죠. 대신 Product Brief를 제시합니다.</p>\n<figure class=\"post__image\" ><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/32/seukeurinsyas-2025-06-28-ojeon-1.43.56.png\" alt=\"\" width=\"1464\" height=\"838\">\n<figcaption >Intel Optane SSD 905P Series - Product Brief</figcaption>\n</figure>\n<p>905P 시리즈 중에서 <a href=\"https://www.intel.co.kr/content/www/kr/ko/products/sku/129833/intel-optane-ssd-905p-series-1-5tb-12-height-pcie-x4-20nm-3d-xpoint/specifications.html\">1.5TB 모델</a>은 이후에 추가로 출시된 제품이라 이 이미지엔 960GB 모델이 최대입니다. 성능에 대해서 <em>Up to</em>를 명시해뒀기에 1.5TB 모델이 섞이면 960GB 모델의 성능이 가려지므로 이것으로 준비했습니다. <a href=\"https://freshflash4096.github.io/benchmark-method/\">Intel ARK</a>에 명시된 부분과 다른게 없는 것을 확인할 수 있습니다.</p>\n<hr>\n<h2 id=\"mcetoc_1iup4vgr6dd\">SW Report</h2>\n<figure class=\"post__image\" ><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/32/CDI_Before_Purge.png\" alt=\"\" width=\"1002\" height=\"692\">\n<figcaption >\bCrystalDiskInfo 9.7.0 - 905P 960GB</figcaption>\n</figure>\n<p>사람들에게 익숙한 CrystalDiskInfo입니다.</p>\n<p>0E의 <em>Media and Data Integrity Errors</em>가 눈에 띕니다. 이는 이전 주인이 PCIe 라이저 등을 이용해서 상당히 괴상한 시스템 빌드를 즐기시는 분이라 발생한 문제로 생각됩니다. 실제로 제가 받은 이후 해당 값은 변하지 않았고요.</p>\n<p>DUT의 정확한 PN은 INTEL SSDPED1D960GAY이며, FW의 버전은 <strong>E2010650</strong>입니다. PN의 마지막 글자가 조금 신경쓰이네요.</p>\n<p>smartmontools와 NVMe-CLI의 id-ctrl 결과는 <a href=\"https://github.com/FreshFlash4096/SSD-Report\">GitHub</a>에 첨부하도록 하겠습니다.</p>\n<h2 id=\"mcetoc_1iup4vgr6dd\">DUT Summary</h2>\n<p>벤치마크를 진행했던 SSD의 정보를 요약하자면 아래와 같습니다.</p>\n<table style=\"border-collapse: collapse; width: 100%; height: 339.938px;\" border=\"1\">\n<tbody>\n<tr style=\"height: 48.5625px;\">\n<td style=\"width: 99.8573%; background-color: #34495e; height: 48.5625px;\" colspan=\"4\"><strong><span style=\"color: #ffffff;\">INTEL SSDPED1D960GAY</span></strong></td>\n</tr>\n<tr style=\"height: 48.5625px;\">\n<td style=\"width: 24.9643%; background-color: #34495e; height: 48.5625px;\"><span style=\"color: #ffffff;\">Link</span></td>\n<td style=\"width: 24.9643%; height: 48.5625px;\">PCIe 3.0 x4</td>\n<td style=\"width: 24.9643%; background-color: #34495e; height: 48.5625px;\"><span style=\"color: #ffffff;\">NVMe Version</span></td>\n<td style=\"width: 24.9643%; height: 48.5625px;\">NVMe 1.1</td>\n</tr>\n<tr style=\"height: 48.5625px;\">\n<td style=\"width: 24.9643%; background-color: #34495e; height: 48.5625px;\"><span style=\"color: #ffffff;\">Firmware</span></td>\n<td style=\"width: 24.9643%; height: 48.5625px;\">E2010650</td>\n<td style=\"width: 24.9643%; background-color: #34495e; height: 48.5625px;\"><span style=\"color: #ffffff;\">LBA Size</span></td>\n<td style=\"width: 24.9643%; height: 48.5625px;\">512B</td>\n</tr>\n<tr style=\"height: 48.5625px;\">\n<td style=\"width: 24.9643%; background-color: #34495e; height: 48.5625px;\"><span style=\"color: #ffffff;\">Controller</span></td>\n<td style=\"width: 24.9643%; height: 48.5625px;\">Intel SLL3D</td>\n<td style=\"width: 24.9643%; background-color: #34495e; height: 48.5625px;\"><span style=\"color: #ffffff;\">Warning Temp</span></td>\n<td style=\"width: 24.9643%; height: 48.5625px;\">-</td>\n</tr>\n<tr style=\"height: 48.5625px;\">\n<td style=\"width: 24.9643%; background-color: #34495e; height: 48.5625px;\"><span style=\"color: #ffffff;\">Storage Media</span></td>\n<td style=\"width: 24.9643%; height: 48.5625px;\">Intel 1st 3DXP</td>\n<td style=\"width: 24.9643%; background-color: #34495e; height: 48.5625px;\"><span style=\"color: #ffffff;\">Critical Temp</span></td>\n<td style=\"width: 24.9643%; height: 48.5625px;\">-</td>\n</tr>\n<tr style=\"height: 48.5625px;\">\n<td style=\"width: 24.9643%; background-color: #34495e; height: 48.5625px;\"><span style=\"color: #ffffff;\">Power State</span></td>\n<td style=\"width: 24.9643%; background-color: #34495e; height: 48.5625px;\"><span style=\"color: #ffffff;\">Maximum Power</span></td>\n<td style=\"width: 24.9643%; background-color: #34495e; height: 48.5625px;\"><span style=\"color: #ffffff;\">Entry Latency</span></td>\n<td style=\"width: 24.9643%; background-color: #34495e; height: 48.5625px;\"><span style=\"color: #ffffff;\">Exit Latency</span></td>\n</tr>\n<tr style=\"height: 48.5625px;\">\n<td style=\"width: 24.9643%; height: 48.5625px;\">PS0</td>\n<td style=\"width: 24.9643%; height: 48.5625px;\">18.00W</td>\n<td style=\"width: 24.9643%; height: 48.5625px;\">0μs</td>\n<td style=\"width: 24.9643%; height: 48.5625px;\">0μs</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"mcetoc_1iuqh3uem5o\">Closing</h2>\n<p>이 정도가 제 SSD 리뷰에서 앞 부분에 올 정보입니다. 큰 내용은 없습니다만, 주(主)라고도 할 수 있는 성능 벤치마크에 앞선 전채요리라고 생각하시면 될 것 같네요.</p>\n<p>Intel MAS로 LED색을 바꾸는 장면도 찍어보려고 했는데 여건상 하기가 좀 곤란했던게 아쉽습니다. SCM급 SSD인데 cSSD 벤치마크만 수행한 것도 조금 한으로 남을 것 같네요. 실제로 Optane SSD 9 시리즈는 cSSD로 하이엔드 소비자가 타겟시장이었으니, eSSD 벤치마크는 P4800X로 한을 메우기로 합니다.</p>\n<p>905P는 슬슬 놓아주고, 현재 리뷰 대기 중인 SSD는 약 25개 정도 있습니다. 대부분이 eSSD라 시간이 상당히 걸릴 것 같네요. 곧 자취를 준비해야하기도 하고... 유튜브 영상으로도 리뷰를 만들어보려고 하는데, 가지고 있는 SSD만 하더라도 언제 끝날지 상상이 안 가네요.</p>\n<p>오늘도 더 열심히 살아야겠다고 다짐합니다.</p>",
            "image": "https://freshflash4096.github.io/media/posts/32/905P_Box_3.jpg",
            "author": {
                "name": "Huie"
            },
            "tags": [
                   "Benchmark"
            ],
            "date_published": "2025-06-27T20:57:28+09:00",
            "date_modified": "2025-06-28T14:59:06+09:00"
        },
        {
            "id": "https://freshflash4096.github.io/benchmark-method/",
            "url": "https://freshflash4096.github.io/benchmark-method/",
            "title": "My SSD Benchmark Method",
            "summary": "Change log 2025.06.25 - 어색한 문맥 수정, Reference 목록 추가 2025.06.28 -&hellip;",
            "content_html": "<blockquote>\n<p>Change log</p>\n<p>2025.06.25 - 어색한 문맥 수정, Reference 목록 추가</p>\n<p>2025.06.28 - Windows기반 벤치마크 상세옵션 명시, 그래프 오류 수정</p>\n</blockquote>\n<p> </p>\n<p>국내외를 불문하고 컴덕들을 위한 커뮤니티가 존재합니다. 그리고 커뮤니티에선 새로운 컴퓨터 부품에 대한 벤치마크가 여럿 올라옵니다.</p>\n<p>예시를 들자면, 국내에선 쿨엔조이, 퀘이사존, 조드가 있으며, 해외에선 TechPowerUp, Tom's Hardware 등이 유명하죠. </p>\n<p>전 상술한 국내 커뮤니티 어느 곳에서도 활동하지 않지만, 최근에 제 SSD 벤치마크를 공유하고자 일부에는 계정을 만들었습니다. </p>\n<hr>\n<p>잡담은 그만하고 본론으로 넘어가겠습니다. 벤치마크 대상이 되는 부품 중에선 저장장치가 당연히 존재하며, SSD의 보급이 이루어진 현재는 소비자에게 가장 흔한 저장장치가 SSD라고 해도 과언이 아닐까합니다.<!-- notionvc: 5b358cef-ff98-48d1-aeeb-e064cca58494 --></p>\n<p>흔히 사용하는 벤치마크 도구로는 CrystalDiskMark, ATTO Disk Benchmark, 국내 한정으로 나래온 더티테스트가 있습니다. 해당 도구들은 사용하기 굉장히 편리하게 설계되어 버튼 한 번만 누른다면 결과값을 바로 출력해주죠. 하지만 결과값의 신뢰성에 대해선 재고해봐야 할 문제입니다.<!-- notionvc: 669d6263-0e8f-49ce-9c46-6ce207ea57a2 --></p>\n<figure class=\"post__image\" ><img loading=\"lazy\" style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\" src=\"https://freshflash4096.github.io/media/posts/31/seukeurinsyas-2025-06-21-ohu-11.10.42.png\" alt=\"\" width=\"2290\" height=\"1328\">\n<figcaption >SSD Performace Change</figcaption>\n</figure>\n<p>SSD의 성능은 변화합니다. 가장 처음의 FOB(Fresh Out of Box)상태에서 Transition 단계를 거쳐 안정적인 Steady State로 향하죠. NAND기반의 모든 저장장치는 이와 같은 단계를 따릅니다.</p>\n<p>eSSD(enterprise SSD)는 일반적으로 데이터시트의 값들도 Steady State가 기준인 경우가 대부분이기에 Pre-Conditioning을 거친 후, Steady State의 성능을 측정하는 것이 옳습니다. 하지만 cSSD(consumer SSD)는 FOB의 성능을 데이터시트에 명시해둡니다. <!-- notionvc: 293e48b7-ecab-42ed-8110-deb5c5954a3c --></p>\n<p>cSSD의 판매대상인 일반 소비자가 진정으로 Steady State를 경험할 일이 많은가에 대해서도 고민해봐야합니다. 하지만, 사람들이 사용하는 SSD의 상태는 FOB와 다른 것이 명백합니다.<!-- notionvc: 30444354-414c-4624-b8f4-db8bf50c72b7 --></p>\n<p>거기에 더해, 현대 cSSD에는 대부분 SLC 캐시라는 메커니즘이 포함되어 있습니다. 초기형 SLC캐시는 단순히 고정된 용량이었지만, 최신형에선 전체 공간을 시뮬레이션하는 경우도 있는가하면, 일부 공간만을 시뮬레이션 하는 경우도 있어 성능 측정이 굉장히 복잡해졌습니다.<!-- notionvc: 03381f3f-2f82-4568-93fe-3f332710244d --></p>\n<p>많은 고민이 있었습니다. 한창 SSD에 관심을 가지기 시작했던 <a href=\"https://huie.tistory.com/47\">22년도</a>부터 이러한 고민을 했던 것 같네요.</p>\n<p>여러 번의 시행착오와 실험을 거쳐 저는 아래와 같은 저만의 벤치마크 방법을 수립했습니다. 급하게 진행해야했던 Intel Optane SSD 905P 960GB를 예시로 들어 기술하겠습니다.</p>\n<div class=\"post__toc\">\n<h3>목차</h3>\n<ul>\n<li><a href=\"#mcetoc_1iuejnn4l47m\">cSSD Benchmarking Method</a>\n<ul>\n<li><a href=\"#mcetoc_1iuejmtca479\">Pre-Conditioning</a></li>\n<li><a href=\"#mcetoc_1iuejmtca47a\">CrystalDiskMark 9.x.x</a></li>\n<li><a href=\"#mcetoc_1iuejmtca47b\">3DMark Storage Benchmark</a></li>\n<li><a href=\"#mcetoc_1iuejqi41492\">SPECworkstation 4.0.0</a></li>\n<li><a href=\"#mcetoc_1iuejqi41494\">Fill Drive</a></li>\n<li><a href=\"#mcetoc_1iuejqi41495\">Sync Performance</a></li>\n<li><a href=\"#mcetoc_1iuejmtca47g\">Low QD Performance by RW Ratio </a></li>\n<li><a href=\"#mcetoc_1iujf14onc8\">Percentile Latency</a></li>\n<li><a href=\"#mcetoc_1iuejmtca47i\">Tail Latency</a></li>\n</ul>\n</li>\n<li><a href=\"#mcetoc_1iueju0sm49b\">How about eSSD Benchmarking?</a></li>\n<li><a href=\"#mcetoc_1iuek3eau49h\">Benchmark Flow</a>\n<ul>\n<li><a href=\"#mcetoc_1iuqi92fd62\">WSAT Test</a></li>\n<li><a href=\"#mcetoc_1iuqi92fd63\">eSSD Test</a></li>\n<li><a href=\"#mcetoc_1iuqi92fd64\">cSSD Test</a></li>\n</ul>\n</li>\n<li><a href=\"#mcetoc_1iuek3eau49h\">Test Platform</a>\n<ul>\n<li><a href=\"#mcetoc_1iuek40fa49k\">Hardware</a></li>\n<li><a href=\"#mcetoc_1iuek4g3749n\">Software</a></li>\n<li><a href=\"#mcetoc_1iujejkk8bn\">Future Improvements</a></li>\n</ul>\n</li>\n<li><a href=\"#mcetoc_1iuel76km4a0\">Closing</a></li>\n<li><a href=\"#mcetoc_1iuel7q734a3\">Reference</a></li>\n</ul>\n</div>\n<hr>\n<h2 id=\"mcetoc_1iuejnn4l47m\">cSSD Benchmarking Method</h2>\n<h3 id=\"mcetoc_1iuejmtca479\">Pre-Conditioning</h3>\n<p>윗 문단에서 언급했지만, SSD에서 FOB 성능은 유저가 실질적으로 체감하기 힘든 부분입니다. 그렇다면 SNIA SSS-PTS에 언급된대로의 Pre-Conditioning을 진행해 Steady State를 측정하는 것이 좋을까요? cSSD에 대해서는 조심스레 무조건 긍정하긴 어려울 것 같다는 의견을 제시합니다. </p>\n<p>일반 소비자의 워크로드는 몇 가지 특징을 따르는데, 그중 눈여겨 볼 부분이 있습니다. 바로 <strong>유휴시간이 상당</strong>하다는 것과 <strong>일부영역에만 활발하게 엑세스</strong>한다는 것입니다. 저는 제 cSSD 벤치마크의 Pre-Conditioning 단계를 아래와 같이 정의했습니다.</p>\n<ol>\n<li>장치를 Purge</li>\n<li>LBA의 200%를 128kiB SEQ Write</li>\n<li>LBA의 앞에서 8GiB를 4kiB RND Write</li>\n<li>사용 중인 공간이 75%가 되도록 LBA 기준으로 뒤에서 25%를 Trim</li>\n</ol>\n<p>그럼, 사용하는 도구로 넘어가겠습니다.</p>\n<h3 id=\"mcetoc_1iuejmtca47a\">CrystalDiskMark 9.x.x</h3>\n<div class=\"gallery-wrapper\"><div class=\"gallery\"  data-is-empty=\"false\" data-translation=\"이미지 추가\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/31/gallery/seukeurinsyas-2025-06-21-235614.png\" data-size=\"1002x547\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/31/gallery/seukeurinsyas-2025-06-21-235614-thumbnail.png\" alt=\"\" width=\"768\" height=\"419\"></a>\n<figcaption>905P 960GB - Default / FOB</figcaption>\n</figure>\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/31/gallery/seukeurinsyas-2025-06-22-001645.png\" data-size=\"1002x547\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/31/gallery/seukeurinsyas-2025-06-22-001645-thumbnail.png\" alt=\"\" width=\"768\" height=\"419\"></a>\n<figcaption>905P 960GB - NVMe / FOB</figcaption>\n</figure>\n</div></div>\n<div class=\"gallery-wrapper\"><div class=\"gallery\"  data-is-empty=\"false\" data-translation=\"이미지 추가\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/31/gallery/seukeurinsyas-2025-06-22-012113.png\" data-size=\"1002x547\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/31/gallery/seukeurinsyas-2025-06-22-012113-thumbnail.png\" alt=\"\" width=\"768\" height=\"419\"></a>\n<figcaption>905P 960GB - Default</figcaption>\n</figure>\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/31/gallery/seukeurinsyas-2025-06-22-012450.png\" data-size=\"1002x547\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/31/gallery/seukeurinsyas-2025-06-22-012450-thumbnail.png\" alt=\"\" width=\"768\" height=\"419\"></a>\n<figcaption>905P 960GB - NVMe</figcaption>\n</figure>\n</div></div>\n<p>가장 대중적인 도구인 CrystalDiskMark. 백엔드의 diskspd를 바로 사용하는 것도 고민했지만, 미리 설정된 프로필을 사용해 사람들에게 쉽게 읽혀지는 것이 목적입니다. FOB상태와 Pre-Conditioning을 끝낸 상태에 대해 기본 프로필과 NVMe 프로필에서 측정합니다. <strong>이후로 제 벤치마크는 별도의 언급이 없다면 FOB상태의 성능은 측정하지 않습니다.</strong></p>\n<h3 id=\"mcetoc_1iuejmtca47b\">3DMark Storage Benchmark</h3>\n<figure class=\"post__image\" ><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/31/seukeurinsyas-2025-06-22-020535.png\" alt=\"\" width=\"1386\" height=\"873\">\n<figcaption >3DMark Storage Benchmark - Capture</figcaption>\n</figure>\n<figure class=\"post__image\" ><img loading=\"lazy\" style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-4.png\" alt=\"\" width=\"1000\" height=\"200\">\n<figcaption >3DMark Storage Benchmark - Score</figcaption>\n</figure>\n<p>게임 성능 벤치마크로 유명한 3DMark의 DLC옵션인 3DMark Storage Benchmark입니다. 이 DLC만을 위해서 그래픽카드도 없는 제가 3DMark를 직접 구매했습니다. CDM과 더불어 다른 리뷰에서도 많이 보이는 벤치마크 도구이기에 자세한 설명은 생략하겠습니다.</p>\n<h3 id=\"mcetoc_1iuejqi41492\">SPECworkstation 4.0.0</h3>\n<figure class=\"post__image\" ><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-3.png\" alt=\"\" width=\"1000\" height=\"200\">\n<figcaption >SPECworkstation - SPEC Ratio</figcaption>\n</figure>\n<p>SPECworkstation은 SPEC(Standard Performance Evaluation Corporation)에서 워크스테이션의 성능 지표를 측정하기 위해 만들어진 벤치마크 도구이며, WPCstorage 워크로드는 스토리지의 성능을 측정합니다. 결과는 SPEC Ratio라는 기준으로 출력되며, <a href=\"https://github.com/SPEC-GWPG-Dev/SPECgwpg-Docs/blob/main/SPECworkstation4/SPECworkstation4-User-Guide.md#reference-machine\">Reference Machine</a>의 성능에 대한 비율로 측정됩니다.</p>\n<h3 id=\"mcetoc_1iuejqi41494\">Fill Drive</h3>\n<figure class=\"post__image\" ><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-2.png\" alt=\"\" width=\"1000\" height=\"500\">\n<figcaption >Fill Drive - BW/TGBW</figcaption>\n</figure>\n<figure class=\"post__image\" ><img loading=\"lazy\" style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-6.png\" alt=\"\" width=\"1000\" height=\"500\">\n<figcaption >Fill Drive - BW/Time</figcaption>\n</figure>\n<figure class=\"post__image\" ><figure class=\"post__image\"><img loading=\"lazy\"  style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-5.png\" alt=\"\" width=\"1000\" height=\"500\"></figure>\n<figcaption >Fill Drive - LAT/Time</figcaption>\n</figure>\n<p>많은 곳에서 볼 수 있는 합성 벤치마크입니다. 국내에서는 나래온 더티테스트를 통해 이러한 벤치마크를 진행하는데, 개인적으로 나래온 더티테스트는 X축이 용량 단위이며, 기록 단위도 용량의 0.1% 단위라는 것이 꽤 큰 단점이라고 생각해 직접 진행하기로 했습니다.</p>\n<p>100ms단위로 측정하며, 워크로드는 SEQ 128kiB QD256입니다. 해당 벤치마크는 목적상 이례적으로 FOB상태에서 진행해 용량의 100%를 채우고 휴식을 부여합니다. 그리고 한 번 더 용량의 100%에 해당하는 쓰기를 가합니다. cSSD에선 이를 통해 SLC캐시의 형태를 대략적으로 파악할 수 있으리라 기대됩니다.</p>\n<p>그래프는 여러 가지가 제시됩니다. 미리 제시된 첫 번째 그래프는 직관적인 이해를 위해 제시했지만, 최신 cSSD처럼 SLC캐시를 적용해 시간이 지남에 따라 변화하는 성능이라면 해상도 분포가 다른 것이 상당한 단점입니다. 두 번째는 평균값에 대한 수평선을 추가했으며, 세 번째는 지연시간을 통해 안정적인 정도를 파악할 수 있습니다. </p>\n<p>이외에 추가될 그래프는 전체 평균 대역폭을 순서대로 정렬한 그래프입니다. 양식은 아래의 Sync Performance 파트에 사용된 막대 그래프와 같습니다.</p>\n<h3 id=\"mcetoc_1iuejqi41495\">Sync Performance</h3>\n<figure class=\"post__image\" ><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1.png\" alt=\"\" width=\"1000\" height=\"200\">\n<figcaption >Sync Performance - SEQ Write BW</figcaption>\n</figure>\n<figure class=\"post__image\" ><img loading=\"lazy\" style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-10.png\" alt=\"\" width=\"1000\" height=\"200\">\n<figcaption >Sync Performance - SEQ Write LAT</figcaption>\n</figure>\n<figure class=\"post__image\" ><img loading=\"lazy\" style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-9.png\" alt=\"\" width=\"1000\" height=\"200\">\n<figcaption >Sync Performance - RND Write BW</figcaption>\n</figure>\n<figure class=\"post__image\" ><img loading=\"lazy\" style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-8.png\" alt=\"\" width=\"1000\" height=\"200\">\n<figcaption >Sync Performance - RND Write LAT</figcaption>\n</figure>\n<p>동기 쓰기, fsync()의 성능을 확인합니다. 일반적으로 경험하는 워크로드는 비동기 쓰기인 경우가 많습니다. 하지만 Homelab에서 ZFS나 Ceph 등을 사용하는 경우, 동기 쓰기 워크로드가 발생하기도 합니다.</p>\n<p>cSSD에서 동기 쓰기는 HW PLP(Power Less Protection)이 구현되지 않아 펌웨어의 차이로 인해 일반적으로 생각하는 쓰기 성능과 차이가 심하게 납니다. 일반적으로 찾아보기 힘든 정보인 부분도 있어 해당 벤치마크를 채용했습니다.</p>\n<p>SEQ 성능은 128kiB QD1으로 1GiB를 작성하며, Pre-Conditioning 단계에서 순차로 작성된 영역에만 접근합니다. LBA 기준으로 8GiB ~ 총 용량의 75% 지점입니다.</p>\n<p>RND 성능은 4kiB QD1으로 1GiB를 작성하며, Pre-Conditioning 단계에서 랜덤으로 작성된 영역에만 <span style=\"text-decoration: underline;\">완전히 랜덤</span>으로 접근합니다. LBA 기준으로 0GiB에서 8GiB 지점입니다. </p>\n<h3 id=\"mcetoc_1iuejmtca47g\">Low QD Performance by RW Ratio </h3>\n<figure class=\"post__image\" ><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-11.png\" alt=\"\" width=\"1200\" height=\"600\">\n<figcaption >SEQ Performance on RW Ratio</figcaption>\n</figure>\n<figure class=\"post__image\" ><img loading=\"lazy\" style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-12.png\" alt=\"\" width=\"1200\" height=\"600\">\n<figcaption >RND Performance on RW Ratio</figcaption>\n</figure>\n<figure class=\"post__image\" ><img loading=\"lazy\" style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-14.png\" alt=\"\" width=\"1200\" height=\"600\">\n<figcaption >SEQ Performance on RW Ratio - Weighted</figcaption>\n</figure>\n<figure class=\"post__image\" ><img loading=\"lazy\" style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-13.png\" alt=\"\" width=\"1200\" height=\"600\">\n<figcaption >RND Performance on RW Ratio - Weighted</figcaption>\n</figure>\n<p>cSSD의 스펙시트는 SEQ RW, RND RW로 이루어진 4-corners performance만 제시합니다. 하지만 일반적인 SSD는 읽쓰기 작업이 혼합되어 있을 때 성능이 하락하며, 100% 읽기 또는 100% 쓰기의 작업이 발생하는 것은 드뭅니다. 따라서 읽기 비율을 10% 단위로 변경하며 성능을 측정합니다.</p>\n<p>SEQ은 128kiB QD1, QD2, QD4로 진행하며 각 128MiB *3의 작업을 수행하고, RND도 4kiB QD1, QD2, QD4로 진행하며 각 4MiB *160의 작업을 수행합니다. 앞의 Sync 성능 측정과 동일하게 SEQ과 RND은 Pre-Conditioning이 진행된 각 영역에만 접근합니다.</p>\n<p>다른 SSD과 비교는 QD1 70%, QD2 20%, QD4 10%로 가중치를 부여한 그래프를 통해 진행합니다. 여기에 더해, 이 벤치마크를 진행하며 아래의 Percentile Latency, Tail Latency 분석에서도 활용되는 정보를 추출합니다.</p>\n<h3 id=\"mcetoc_1iujf14onc8\">Percentile Latency</h3>\n<figure class=\"post__image\" ><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-15.png\" alt=\"\" width=\"1000\" height=\"600\">\n<figcaption >RND Read - Percentile/LAT</figcaption>\n</figure>\n<figure class=\"post__image\" ><img loading=\"lazy\" style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-18.png\" alt=\"\" width=\"1000\" height=\"600\">\n<figcaption >RND Write - Percentile/LAT</figcaption>\n</figure>\n<p>기본적으로 cSSD의 스펙시트에는 지연시간을 잘 나타내지 않습니다. 하지만 실질적으로 사용자가 체감하게 되는 것은 프로그램들을 실행시킨것이나 어떤 명령에 대한 지연시간입니다. 이러한 지연시간을 백분위수로 나타낸 그래프입니다. SEQ 작업은 특성상 작업이 동시에 완전히 종료되는 것이 중요하다고 판단했기에, RND 성능, 그 중에서도 QD1에 대해서만 비교합니다.</p>\n<p>이 그래프들은 지연시간을 나타내지만 실질적인 IOPS는 나타내지 않습니다. 타당한 비교를 하기 위해서는 지연시간만이 아닌 별도의 성능지표가 필요합니다. 따라서 평균 IOPS를 범례에 표시해 여러 SSD를 비교합니다.</p>\n<p><span style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\">다른 파트에서는 눈에 익숙한 MiB/s값을 고집했지만, 여기서는 IOPS단위를 고집한 이유는 다음과 같습니다. MiB/s 단위로 표기하면 변화가 작아서 덜 직관적이라는 것이 첫 번째이며, 이 부분의 그래프와 다음 부분의 그래프의 워크로드는 전부 4kiB 사이즈란 것이 두 번째입니다.</span></p>\n<h3 id=\"mcetoc_1iuejmtca47i\">Tail Latency</h3>\n<figure class=\"post__image\" ><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-19.png\" alt=\"\" width=\"1000\" height=\"600\">\n<figcaption >RND Read - Tail Latency</figcaption>\n</figure>\n<figure class=\"post__image\" ><img loading=\"lazy\" style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-21.png\" alt=\"\" width=\"1000\" height=\"600\">\n<figcaption >RND RW70 - Tail Latency</figcaption>\n</figure>\n<figure class=\"post__image\"><img loading=\"lazy\"  style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\" src=\"https://freshflash4096.github.io/media/posts/31/Figure_1-22.png\" alt=\"\" width=\"1000\" height=\"600\"></figure>\n<table style=\"border-collapse: collapse; width: 100%; height: 291.375px;\" border=\"1\">\n<tbody>\n<tr style=\"height: 48.5625px;\">\n<td style=\"width: 24.9644%; height: 48.5625px;\">Percentile</td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">RND Read</td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">RND Read 70%</td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">RND Write</td>\n</tr>\n<tr style=\"height: 48.5625px;\">\n<td style=\"width: 24.9644%; height: 48.5625px;\">90%</td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">10.98 <span style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\">µs</span></td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">22.67 µs</td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">22.97 µs</td>\n</tr>\n<tr style=\"height: 48.5625px;\">\n<td style=\"width: 24.9644%; height: 48.5625px;\">99%</td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">20.45 <span style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\">µs</span></td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">23.72 µs</td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">36.10 µs</td>\n</tr>\n<tr style=\"height: 48.5625px;\">\n<td style=\"width: 24.9644%; height: 48.5625px;\">99.9%</td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">38.75 <span style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\">µs</span></td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">43.80 µs</td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">48.57 µs</td>\n</tr>\n<tr style=\"height: 48.5625px;\">\n<td style=\"width: 24.9644%; height: 48.5625px;\">99.99%</td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">43.62 <span style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\">µs</span></td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">50.15 µs</td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">52.31 µs</td>\n</tr>\n<tr style=\"height: 48.5625px;\">\n<td style=\"width: 24.9644%; height: 48.5625px;\">99.999%</td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">52.27 <span style=\"color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);\">µs</span></td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">57.57 µs</td>\n<td style=\"width: 24.9644%; height: 48.5625px;\">53.83 µs</td>\n</tr>\n</tbody>\n</table>\n<p>꼬리 지연시간입니다. eSSD의 스펙시트에서는 QoS 항목에 이러한 정보가 명시되어 있습니다.</p>\n<p>사실상 Percentile Latency 그래프에서 90% 이상 구간을 확대시키고 로그스케일을 적용한 그래프입니다. 총 IO에서 가장 늘어진 지연시간들의 집합이며, 사용자 경험에 직접 영향을 줄 수 있는 영역입니다.</p>\n<p>Percentile Latency, Tail Latency에서 이러한 X축과 Y축을 채용한 것은 Y=X에 가까운 그래프면 조금은 더 파악하기 쉽다는 발상에서 착안했습니다. one-nine Latency부터 five-nine Latency에 대한 상세한 값은 별도의 표로 나타냅니다.</p>\n<hr>\n<h2 id=\"mcetoc_1iueju0sm49b\">How about eSSD Benchmarking?</h2>\n<p>eSSD 벤치마크는 ezFIO를 통해 수행되며, eSSD 중에서도 SCM급이나 WI SSD는 추가로 WSAT(Write Saturation) Test를 수행합니다. </p>\n<p>다만, 최신 초고용량 SSD에서는 IU(Indirection Unit)의 크기가 4k가 아닌 경우가 있는데, 이러한 경우에는 드라이브의 IU보다 작은 크기의 쓰기 벤치마크는 진행하지 않습니다. </p>\n<table style=\"border-collapse: collapse; width: 100%;\" border=\"1\">\n<tbody>\n<tr>\n<td style=\"width: 24.9644%;\"> </td>\n<td style=\"width: 24.9644%;\">cSSD</td>\n<td style=\"width: 24.9644%;\">eSSD - RI / Mix</td>\n<td style=\"width: 24.9644%;\">eSSD - WI</td>\n</tr>\n<tr>\n<td style=\"width: 24.9644%;\">cSSD Benchmark</td>\n<td style=\"width: 24.9644%;\">O</td>\n<td style=\"width: 24.9644%;\">O</td>\n<td style=\"width: 24.9644%;\">O</td>\n</tr>\n<tr>\n<td style=\"width: 24.9644%;\">eSSD Benchmark</td>\n<td style=\"width: 24.9644%;\">X</td>\n<td style=\"width: 24.9644%;\">O</td>\n<td style=\"width: 24.9644%;\">O</td>\n</tr>\n<tr>\n<td style=\"width: 24.9644%;\">WSAT Test</td>\n<td style=\"width: 24.9644%;\">X</td>\n<td style=\"width: 24.9644%;\">X</td>\n<td style=\"width: 24.9644%;\">O</td>\n</tr>\n</tbody>\n</table>\n<p>위 표를 보면 각 SSD에 대해 적용할 벤치마크가 이해가 쉬울 것입니다. 몇 가지 궁금할 사항에 대해 설명하겠습니다.</p>\n<p>eSSD에 대해 cSSD 벤치마크를 수행하는 이유는 일반 소비자들도 드물지만 중고시장을 통해 eSSD를 구매하는 경우가 있기 때문입니다. 일반 소비자의 워크로드에 최적화되진 않았지만 저를 포함한 사용사례가 분명히 존재하며, 다른 리뷰에선 찾아보기 힘들기 때문에 제 궁금사항을 풀고자 진행합니다.</p>\n<p>그렇다면 cSSD에 대해 eSSD 벤치마크를 수행하지 않는 이유는? cSSD에서는 eSSD 벤치마크의 기본이 되는 Steady State에 진입하는 것부터가 힘든 사례가 일부 존재합니다. 애초부터 시작할 수도 없는 SSD가 있다는 말이죠.</p>\n<p>물론 가능한 SSD들도 상당수가 존재하지만, 일반적으로 도입하기엔 시간이 상당히 오래 걸리게되어 타협할 수 밖에 없는 선택지였습니다. 이러한 이유로 cSSD 벤치마크에서도 계획보다 제외된 테스트가 상당수 존재합니다.</p>\n<hr>\n<h2 id=\"mcetoc_1iuek3eau49h\">Benchmark Flow</h2>\n<p>방법을 정리하며 흐름을 살펴보겠습니다. 각 테스트 집합은 완전히 개별로 진행됩니다. </p>\n<h3 id=\"mcetoc_1iuqi92fd62\">WSAT Test</h3>\n<p>FOB상태에서 진행되며, User Capacity의 4배 용량만큼 4kiB RND 쓰기를 가하는 것과 512B SEQ 쓰기를 가하는 것으로 진행됩니다. 후자는 RW 비율이 50:50로 진행됩니다.</p>\n<h3 id=\"mcetoc_1iuqi92fd63\">eSSD Test</h3>\n<p>앞서 말한대로 SNIA의 SSS-PTS에 입각해 Purge를 진행한 후, WIPC(Workload Independent Pre-Conditioning)으로 128kiB SEQ 쓰기를 전체 용량에 대해 2번 진행하고 WDPC(Workload Dependent Pre-Conditioning)으로 벤치마크할 워크로드에 대해서 Steady State에 도달할 때까지 부하를 가합니다.</p>\n<p>Steady State에 도달하면 측정값을 수집하는데, 이러한 일련의 과정에는 휴식이 부여되지 않습니다.</p>\n<h3 id=\"mcetoc_1iuqi92fd64\">cSSD Test</h3>\n<p>종류가 많으므로 단계를 나열하겠습니다.</p>\n<ol>\n<li>Purge</li>\n<li>FOB상태에서 CDM Default, NVMe Profile의 순서로 벤치마크 수행</li>\n<li>cSSD Pre-Conditioning 진행 이후, CDM, 3DMark, SPECworkstation 벤치마크 수행</li>\n<li>Purge</li>\n<li>Fill Drive 벤치마크 수행</li>\n<li>cSSD Pre-Conditioning 중 RND와 Trim 단계 진행 후, 앞에서 말한 벤치마크를 차례대로 수행</li>\n</ol>\n<p>Fill Drive 벤치마크 자체가 cSSD Pre-Conditioning에서 SEQ영역을 작성하는 것과 같으므로 두 번째 Pre-Conditioning에선 SEQ 단계를 제외합니다.</p>\n<p>또한, cSSD Pre-Conditioning은 eSSD와 다르게 유휴시간이 충분히 주어집니다. 뿐만 아니라, 성능 측정 단계에서도 충분한 유휴시간이 주어지는 것이 중요한 부분입니다.</p>\n<hr>\n<h2 id=\"mcetoc_1iuek3eau49h\">Test Platform</h2>\n<p>벤치마크 종류를 살펴보았으니 이제 수행되는 플랫폼에 대해 살펴보겠습니다. 금전의 영향을 많이 받아 타협을 거듭했습니다.</p>\n<h3 id=\"mcetoc_1iuek40fa49k\">Hardware</h3>\n<table style=\"border-collapse: collapse; width: 100%;\" border=\"1\">\n<tbody>\n<tr>\n<td style=\"width: 17.2611%;\"><strong>CPU</strong></td>\n<td style=\"width: 64.4789%;\">AMD Ryzen 5 9600X</td>\n<td style=\"width: 18.26%;\">PBO Disable</td>\n</tr>\n<tr>\n<td style=\"width: 17.2611%;\"><strong>MainBoard</strong></td>\n<td style=\"width: 64.4789%;\">ASRock B650M PG Riptide WiFi White</td>\n<td style=\"width: 18.26%;\">BIOS 3.30</td>\n</tr>\n<tr>\n<td style=\"width: 17.2611%;\"><strong>RAM</strong></td>\n<td style=\"width: 64.4789%;\">TeamGroup T-create DDR5-5600 CL46 Classic 32GB *2</td>\n<td style=\"width: 18.26%;\"> </td>\n</tr>\n<tr>\n<td style=\"width: 17.2611%;\"><strong>Boot SSD</strong></td>\n<td style=\"width: 64.4789%;\">Intel Optane SSD 800P 118GB</td>\n<td style=\"width: 18.26%;\"> </td>\n</tr>\n<tr>\n<td style=\"width: 17.2611%;\"><strong>PSU</strong></td>\n<td style=\"width: 64.4789%;\">Corsair SF750 ATX3.1</td>\n<td style=\"width: 18.26%;\"> </td>\n</tr>\n<tr>\n<td style=\"width: 17.2611%;\"><strong>Case</strong></td>\n<td style=\"width: 64.4789%;\">SilverStone RM23-502-MINI</td>\n<td style=\"width: 18.26%;\"> </td>\n</tr>\n<tr>\n<td style=\"width: 17.2611%;\"><strong>Cooler</strong></td>\n<td style=\"width: 64.4789%;\">Noctua NH-L9a-AM5 chromax.black</td>\n<td style=\"width: 18.26%;\">Full Speed</td>\n</tr>\n<tr>\n<td style=\"width: 17.2611%;\"><strong>Cooler</strong></td>\n<td style=\"width: 64.4789%;\">Noctua NF-A8 PWM</td>\n<td style=\"width: 18.26%;\">Full Speed</td>\n</tr>\n<tr>\n<td style=\"width: 17.2611%;\"><strong>Ex</strong></td>\n<td style=\"width: 64.4789%;\">Sipeed NanoKVM PCIe</td>\n<td style=\"width: 18.26%;\"> </td>\n</tr>\n<tr>\n<td style=\"width: 17.2611%;\"><strong>SW_Win</strong></td>\n<td style=\"width: 64.4789%;\">Windows 11 Pro (24H2 26100.4351)</td>\n<td style=\"width: 18.26%;\"> </td>\n</tr>\n<tr>\n<td style=\"width: 17.2611%;\"><strong>SW_Linux</strong></td>\n<td style=\"width: 64.4789%;\">Rocky Linux 10 Minimal (6.12.0-55.12.1.el10_0)</td>\n<td style=\"width: 18.26%;\">FIO 3.36</td>\n</tr>\n</tbody>\n</table>\n<p>DUT(Device Under Test)가 될 SSD는 CPU레인에 직결하며, 최대 2-connector Topology까지 허용합니다. </p>\n<p>다시 말해, M.2 NVMe SSD는 CPU와 가장 가까운 PCIe 5.0 x4를 지원하는 M.2 슬롯에 장착하며, AIC타입과 2.5\" NVMe SSD는 CPU와 가장 가까운 PCIe x16 슬롯에 장착하게 됩니다. 물론, 후자는 어댑터의 사용이 필수불가결하며, 여기엔 <span style=\"text-decoration: underline;\"><em>UMC-PTU-2</em></span>라는 어댑터를 사용합니다.</p>\n<p>2.5\" SATA SSD는 PROM21(B650 Chipset)에 내장된 SATA 컨트롤러에 의지해, 온보드 SATA 포트에 장착합니다. PCH 드라이버 버전은 <strong>7.03.21.2116</strong>입니다.</p>\n<p>기본적으로 이 시스템은 제 랩이 가동되는 랙에 마운트되어 있기에 원격으로 설정할 필요가 있습니다. 따라서, <em><span style=\"text-decoration: underline;\">NanoKVM</span></em>을 사용해 OOBM(Out-of-band Management)기능을 더합니다.</p>\n<p>기본적으로 DUT는 Spot Cooling을 통해 Thermal Throttling을 방지합니다.</p>\n<h3 id=\"mcetoc_1iuek4g3749n\">Software</h3>\n<p>Windows는 전원옵션에서 고성능을 설정하고 상세값을 그에 맞춰주었으며 Indexing Service, Scheduled Defragmentation, System Protection, Windows Defender, Windows Updates를 비활성화했습니다. 혹시 모를 백그라운드 작업에 대비해 네트워크는 연결하지 않고 벤치마크를 진행합니다.</p>\n<pre class=\"language-batch\"><code>start /wait Rundll32.exe advapi32.dll/ProcessIdleTasks</code></pre>\n<p>그에 더해 HWiNFO, 작업관리자, 벤치마크 도구를 실행시킨 뒤, 위 명령어를 입력하고 15분 뒤를 IDLE상태로 정의해 벤치마크를 진행했습니다.</p>\n<p>Windows는 Linux와 달리 NTFS의 기본값으로 포맷한 뒤에 벤치마크를 진행합니다. Rocky Linux는 Minimal 옵션으로 설치, 이후에 필요한 패키지만 추가해 사용하고 드라이버는 Windows와 Linux. 둘 다 Inbox Driver를 사용했습니다. </p>\n<p>Latency는 IO 조각이 제출된 시점에서 완료될 때까지의 시간을 측정한 값을 사용합니다.</p>\n<h3 id=\"mcetoc_1iujejkk8bn\">Future Improvements</h3>\n<p>자본은 유한합니다. 금전 뿐만 아니라 시간도 하나의 자본이죠.</p>\n<p>최초 계획과 현재 방법에는 약간의 차이가 있는데, 특히 cSSD 벤치마크에서 가장 차이가 벌어집니다. 유휴시간에 대한 캐시 복구, Steady State 성능, OCP BootBench등도 시간 상의 문제로 누락했습니다. 이는 추후에도 일반적으로 도입할 생각은 없으며, 몇몇 특이한 사항이 있는 SSD에 한정해 별도로 추가 벤치마크를 진행할 가능성이 있습니다.</p>\n<p>SSD에서 중요한 지표는 성능 뿐만이 아니라 신뢰성, 전력효율, 발열 등이 있습니다. 신뢰성은 개인의 입장에서 굉장히 평가하기 어려운 지표이므로 진작 포기했지만 전력효율과 발열에 대해선 생각이 조금 있습니다.</p>\n<p>전력은 개인적으로 Quarch Technology사의 제품을 고민 중인데, 가격이 상당해 마음에만 담아두고 있는 상태입니다. ElmorLabs에도 PMD라는 전력측정 장비가 있긴하지만, cSSD의 유휴 전력을 제대로 측정할 수 있는지 약간 의문이 들어 보류했습니다.</p>\n<p>온도는 쉽게 측정하려면 S.M.A.R.T.를 쿼리하며 로그를 남기는 방식으로 아주 쉬울 수 있습니다. 하지만 제 환경에서는 여러 의미로 변인이 통제되지 않는데, 우선 테스트 시스템 자체가 별도의 시스템이 추가로 들어있는 랙 캐비닛에 수납되어 있기에 온도 통제에서 자유롭지 못합니다. 또한, SSD에 내장된 온도센서도 조금씩 차이가 있기에 외부 온도계를 사용해야 정확한 비교가 가능하다고 판단했습니다.</p>\n<p>추가로 HW측면에서는 이후 쿨링이나 편의성을 고려해 2U 케이스에서 3U 케이스로 변경할 생각이며, 2.5\" NVMe SSD의 테스트에는 최소한 ReDriver가 사용된 어댑터를 사용하고자 합니다. 추가로 메인보드 또한 온보드 BMC가\b구현된 보드를 생각하고 있습니다.</p>\n<p>부팅용 SSD는 Samsung PM981a 256GB로 2개를 마련해놓은 상태이며, 각자 OS를 설치해 바꿔가며 벤치마크를 진행할 예정입니다. M.2 단자의 수명이 약간은 걱정되지만 이것이 자금을 생각할 때 가장 나은 방법이라고 판단했습니다.</p>\n<h2 id=\"mcetoc_1iuel76km4a0\">Closing</h2>\n<p>여기까지 오는데 많은 시간이 걸린 것 같습니다. 다만, 예시를 905P로 들게된 점은 약간 아쉽기도 합니다. 개인적인 이유로 제 손에서 떠날 물건이라 빠르게 진행했네요.</p>\n<p>우선, 지인들이 빌려준 cSSD들이 상당 수 있습니다. 얼른 벤치마크를 진행하고 돌려드리려 합니다. eSSD 밖에 없어서 첫 벤치마크 대상으로 고민하던 제게 희망을 제시한 지인분들께 감사를 전합니다.</p>\n<figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://freshflash4096.github.io/media/posts/31/seukeurinsyas-2025-06-24-ojeon-12.11.30.png\" alt=\"\" width=\"309\" height=\"102\"></figure>\n<p>코드는 비공개 레포에서 개발했으며, 구성비율은 위와 같습니다. 앞으로 개선사항은 있더라도 코드 자체를 공개할 생각은 없습니다.</p>\n<p>모르는 부분이 많고 아는 것도 없지만 직접 벤치마크를 하려는 이유는 제가 궁금한 점을 중심으로 벤치마크를 진행한 곳이 없었기 때문입니다. 저와 비슷하게 궁금한 것이 있는 분들에게 제 벤치마크가 도움이 되었으면 합니다.</p>\n<p>컴퓨터에 관심을 가지게 된지 6년이 되어갑니다. 6년 뒤에도 벤치마크를 하고 있을지는 모르겠지만, 이렇게 공개적으로 하며 저도 성장해갔으면 좋겠네요.</p>\n<p>긴 글 읽어주셔서 감사합니다.</p>\n<hr>\n<h2 id=\"mcetoc_1iuel7q734a3\">Reference</h2>\n<pre>제가 고민하며 참고한 자료들의 극히 일부를 순서없이 랜덤으로 정렬한 것 입니다. 이외에도 FMS의 발표자료, 여러 SSD의 데이터시트를 참고했습니다. 문서의 원본을 찾게된다면 이후에도 추가될 수 있습니다.</pre>\n<ul>\n<li>OCP Datacenter NVMe SSD Specification</li>\n<li>ezFIO User Guide</li>\n<li>OCP Hyperscale NVMe Boot SSD Specification</li>\n<li>JEDEC STANDARD Solid-State Drive (SSD) Requirements and Endurance Test Method (JESD218)</li>\n<li>JEDEC STANDARD Solid-State Drive (SSD) Endurance Workloads (JESD219)</li>\n<li>OCP NVMe Cloud SSD Specification</li>\n<li>Intel Performance Benchmarking for PCIe* and NVMe* Enterprise Solid-State Drives</li>\n<li>Solidigm PC Storage Performance in the Real World</li>\n<li>Micron SSD Performance States</li>\n<li>Intel Partition Alignment of Intel SSDs for Achieving Maximum Performance and Endurance</li>\n<li>Understanding SSD Performance: Using the SSS PTS to Evaluate and Compose SSD Performance</li>\n<li>Understanding SSD Performance Using the SNIA SSS PTS Performance Test Specification</li>\n<li>SNIA Solid State Storage Performance Test Secificatoin</li>\n<li>fio - Flexible I/O tester rev. 3.38</li>\n<li>Understanding datacentre workload quality of service</li>\n<li>JEDEC MasterTrace_128GB-SSD</li>\n<li>JEDEC TestTrace_64GB-128GB-SSD</li>\n<li>Intel Optane Solid State Drives for Client Evaluation Guide</li>\n<li>Intel Optane Solid State Drive DC P4800X (Linux) Performance Evaluation Guide</li>\n<li>NVM Express Base Specification</li>\n<li>Intel Optane Solid State Drive DC P4800X Series (Windows) Performance Evaluation Guide</li>\n<li>Intel Optane Solid State Drive DC P4800X (VMware) Performance Evaluation Guide</li>\n</ul>\n<p> </p>",
            "author": {
                "name": "Huie"
            },
            "tags": [
                   "Benchmark"
            ],
            "date_published": "2025-06-22T22:20:00+09:00",
            "date_modified": "2025-06-28T22:47:06+09:00"
        },
        {
            "id": "https://freshflash4096.github.io/first-sale-of-lpcamm2-memory/",
            "url": "https://freshflash4096.github.io/first-sale-of-lpcamm2-memory/",
            "title": "LPCAMM2 메모리의 첫 판매",
            "summary": "Crucial 64GB LPCAMM2 LPDDR5X-7500 memory 이전에 SO-DIMM의 한계를 벗어나기 위해 Dell에서 개발을&hellip;",
            "content_html": "<figure class=\"post__image\" ><figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://freshflash4096.github.io/media/posts/23/Capture-2024-05-08-224425.png\" alt=\"\" width=\"2552\" height=\"1648\"></figure>\n<figcaption >Crucial 64GB LPCAMM2 LPDDR5X-7500 memory</figcaption>\n</figure>\n<p>이전에 SO-DIMM의 한계를 벗어나기 위해 Dell에서 개발을 시작하고 JEDEC에 받아들여져 표준으로 정착된 CAMM2와 LPCAMM2. 여러번 공개되었지만 실제로 판매를 시작한 것은 이번이 처음이다.</p>\n<p>2024년 5월 8일 현재, LPDDR5X-7500으로 32GB와 64GB 모듈이 <a href=\"https://www.crucial.com/memory/ddr5/ct64g75c2lp5xg\">판매</a>되고 있다. 가격은 각 $174.99와 $329.99로 나쁘진 않아보인다.</p>\n<p>문제가 아닌 문제라면, 현재는 초창기라 Lenovo ThinkPad P1 Gen 7만 지원한다. 차츰차츰 CAMM2를 지원하는 노트북이 늘어날 것으로 보이긴 하지만 빠르게 보급이 되었으면 하는 마음이다.</p>\n<p>iFixit과 협력해 교체 방법을 담은 <a href=\"https://ko.ifixit.com/Guide/Lenovo+ThinkPad+P1+Gen+7+LPCAMM2+Memory+Replacement/172267\">메뉴얼</a>을 내놓기도 했는데, 이를 통해 간접적으로 주의해야할 부분을 알 수 있다. 당연하지만 접점에 대해서 만지지 않는 등의 주의를 해야한다고 한다.</p>\n<p>CAMM2에 대한 자세한 정보는 <a href=\"https://www.jedec.org/document_search?search_api_views_fulltext=jesd318\">JESD318</a>를 통해 알 수 있다.</p>\n<hr>\n<h3>참고자료</h3>\n<ul>\n<li><a href=\"https://investors.micron.com/news-releases/news-release-details/micron-delivers-crucial-lpcamm2-lpddr5x-memory-new-ai-ready\">Micron Press Release</a></li>\n<li><a href=\"https://www.crucial.com/memory/ddr5/ct64g75c2lp5xg\">Crucial Shop</a></li>\n<li><a href=\"https://ko.ifixit.com/Guide/Lenovo+ThinkPad+P1+Gen+7+LPCAMM2+Memory+Replacement/172267\">How to Memory Replacement [iFixit]</a></li>\n</ul>",
            "image": "https://freshflash4096.github.io/media/posts/23/Crucial-64GB-LPCAMM2-LPDDR5X-7500-memory.png",
            "author": {
                "name": "Huie"
            },
            "tags": [
                   "Archive"
            ],
            "date_published": "2024-05-08T22:31:25+09:00",
            "date_modified": "2025-06-20T22:56:24+09:00"
        },
        {
            "id": "https://freshflash4096.github.io/cryogenic-3d-nand-etching/",
            "url": "https://freshflash4096.github.io/cryogenic-3d-nand-etching/",
            "title": "극저온 3D NAND 식각",
            "summary": "Tokyo Electron(TEL)은 2023년 6월, IEEE Symposium on VLSI Technology &amp; Circuits에서 Beyond&hellip;",
            "content_html": "<p>Tokyo Electron(TEL)은 2023년 6월, IEEE Symposium on VLSI Technology &amp; Circuits에서 <span class=\"notion-enable-hover\" data-token-index=\"0\"><strong>Beyond 10um Depth Ultra-High Speed Etch Process with 84% Lower Carbon Footprint for Memory Channel Hole of 3D NAND Flash over 400 Layers</strong>를 발표했다.</span><!-- notionvc: 1482f45e-daca-4eba-9e05-da86244ef2c0 --><!-- notionvc: cb9299e0-e391-41c2-94ab-b27f5e88167e --></p>\n<p>해당 발표에선 극저온에서 높은 속도의 식각을 가지는 시스템을 구현했다. 아래에 논문의 Abstract를 인용한다.</p>\n<blockquote>\n<p>A novel High-Aspect-Ratio (HAR) dielectric etch technology which is capable of etching beyond <span id=\"MathJax-Element-2-Frame\" class=\"MathJax\" tabindex=\"0\"><span id=\"MathJax-Span-8\" class=\"math\"><span id=\"MathJax-Span-9\" class=\"mrow\"><span id=\"MathJax-Span-10\" class=\"mn\">10</span><span id=\"MathJax-Span-11\" class=\"mi\">μ</span><span id=\"MathJax-Span-12\" class=\"texatom\"><span id=\"MathJax-Span-13\" class=\"mrow\"><span id=\"MathJax-Span-14\" class=\"mi\">m</span></span></span></span></span></span> depth memory channel hole for future generations of 3D NAND flash memory has been successfully developed for the first time. Ten micron depth etching is not practical with conventional etching technology, but our novel technology using cryogenic wafer stage and new gas chemistry can achieve not only <span id=\"MathJax-Element-3-Frame\" class=\"MathJax\" tabindex=\"0\"><span id=\"MathJax-Span-15\" class=\"math\"><span id=\"MathJax-Span-16\" class=\"mrow\"><span id=\"MathJax-Span-17\" class=\"mn\">10</span><span id=\"MathJax-Span-18\" class=\"mi\">μ</span><span id=\"MathJax-Span-19\" class=\"texatom\"><span id=\"MathJax-Span-20\" class=\"mrow\"><span id=\"MathJax-Span-21\" class=\"mi\">m</span></span></span></span></span></span> etch capability but also quite short process time (33 minutes) with 84% carbon footprint reduction of greenhouse gases. Etched profile was also confirmed to be excellent. Thus, this is a key technology for highly productive, cost effective and sustainable manufacturing of 3D NAND flash memory device.</p>\n</blockquote>\n<p>400L 이상의 3D NAND에서 Single-Stack 또는 Double-Stack의 구축이 가능한 것으로도 기대되는데, 이러한 기술이 적용된 장비를 삼성에서는 직접 테스트하고 있으며, 하이닉스는 TEL에 웨이퍼를 보내 테스트를 진행하는 중이라 한다.</p>\n<p>참고로 작년에 공개된 하이닉스의 321L 3D NAND는 Triple-Stack으로 구현되었으며, 삼성에서도 V10 이후부터는 Triple-Stack을 도입할 것이라 말한 적이 있다.</p>\n<hr>\n<h3>참고자료</h3>\n<ul>\n<li><a href=\"https://www.thelec.kr/news/articleView.html?idxno=27560\">삼성과 하이닉스의 식각 장비 도입에 대한 기사 [THEELEC]</a></li>\n<li><a href=\"https://www.tel.com/news/product/2023/20230609_001.html\">TEL News Room</a></li>\n<li><span class=\"notion-enable-hover\" data-token-index=\"0\"><a href=\"https://ieeexplore.ieee.org/document/10185160\">Beyond 10um Depth Ultra-High Speed Etch Process with 84% Lower Carbon Footprint for Memory Channel Hole of 3D NAND Flash over 400 Layers [IEEE]</a></span></li>\n</ul>",
            "image": "https://freshflash4096.github.io/media/posts/22/Tokyo_Electron_logo.svg.png",
            "author": {
                "name": "Huie"
            },
            "tags": [
                   "Archive"
            ],
            "date_published": "2024-05-07T22:14:53+09:00",
            "date_modified": "2025-06-20T22:56:03+09:00"
        },
        {
            "id": "https://freshflash4096.github.io/cheyenne-auctions/",
            "url": "https://freshflash4096.github.io/cheyenne-auctions/",
            "title": "Cheyenne 경매",
            "summary": "Cheyenne는 2016년 하반기에 설치가 되었으며, 2017년에 가동을 시작해 2023년 말에 가동을 종료했다.",
            "content_html": "<p>Cheyenne는 2016년 하반기에 설치가 되었으며, 2017년에 가동을 시작해 2023년 말에 가동을 종료했다. 최대 5.34 PFLOPs의 성능을 가진 이 컴퓨터는 2016년 말을 기준으로 20위의 성능을 평가받았다. 물론 지금은 100위 밑으로 떨어졌겠지만 말이다.</p>\n<p>총 4,032개의 node로 구성되어 있으며, 각 node에는 <a href=\"https://www.intel.com/content/www/us/en/products/sku/91755/intel-xeon-processor-e52697-v4-45m-cache-2-30-ghz/specifications.html\">E5-2697v4</a>가 2개씩 탑재되었다. 메모리의 용량은 node 별로 다르며, 모두 합치면 313TB의 용량이라고 한다.</p>\n<p>중요한 것은 사양이 아니고 이 친구가 <a href=\"https://gsaauctions.gov/auctions/preview/282996\">경매</a>로 등장했다는 거다. 케이블들은 제외 대상이지만, 스위치 등은 포함된다.</p>\n<p>문제가 있다면... 메모리 관련 문제와 냉각수가 새는 것? 같다. </p>\n<p>하여튼 해당 경매 사이트에는 해당 컴퓨터의 소개와 랙의 무게 등이 기입된 문서와 사진들이 있으니 한 번 살펴보는 것도 재밌을 것이라 생각된다.</p>\n<p>현재 이 글을 작성하는 시각(2024. 05. 02. 22:59 - KST)을 기준으로 입찰가는 $100,170이다. 누가 사가는 것일까?</p>\n<hr>\n<h3>참고자료</h3>\n<ul>\n<li><a href=\"https://www.servethehome.com/leaky-hpe-sgi-cheyenne-supercomputer-for-sale-at-perhaps-a-deal-intel-supermicro-mellanox/\">ServeTheHome의 기사</a></li>\n<li><a href=\"https://www.cisl.ucar.edu/ncar-supercomputing-history/cheyenne\">Cheyenne 소개</a></li>\n</ul>",
            "image": "https://freshflash4096.github.io/media/posts/20/Cheyenne.png",
            "author": {
                "name": "Huie"
            },
            "tags": [
                   "Archive"
            ],
            "date_published": "2024-05-02T22:50:16+09:00",
            "date_modified": "2025-06-20T22:54:41+09:00"
        },
        {
            "id": "https://freshflash4096.github.io/complete-copprlink-spec/",
            "url": "https://freshflash4096.github.io/complete-copprlink-spec/",
            "title": "CopprLink 사양 완성",
            "summary": "2025년, PCI-SIG에선 내부와 외부 PCIe 연결을 지원하기 위해 OCuLink WG(Workgroup)이 만들어졌다. 지금은&hellip;",
            "content_html": "<p>2025년, PCI-SIG에선 내부와 외부 PCIe 연결을 지원하기 위해 OCuLink WG(Workgroup)이 만들어졌다. 지금은 OCuLink WG이 종료된 상태이며, 시장에선 OCuLink 솔루션들이 종종 보인다.</p>\n<p>OCuLink는 과거의 산물... PCIe 5.0과 6.0의 요구사항은 진화했고 이에 부합하는 Cabling에 대한 표준을 개발하게 되었는데, 이것이 CopprLink다. </p>\n<p>위 이미지에서 볼 수 있듯이 CopprLink는 내부와 외부에 따라 별개의 사양을 보여준다.</p>\n<p>간단하게 정리하자면...</p>\n<ul>\n<li>내부 케이블은 SNIA의 <a href=\"https://members.snia.org/document/dl/33768\">SFF-TA-1016</a> 커넥터를 이용하며, 최대 길이는 1m</li>\n<li>외부 케이블은 SNIA의 <a href=\"https://members.snia.org/document/dl/37618\">SFF-TA-1032</a> 커넥터를 이용하며, 최대 길이가 2m</li>\n</ul>\n<p>이렇게 정리할 수 있겠다. 현재는 PCIe 5.0과 6.0을 지원하지만 7.0까지의 확장을 염두에 두고 있다. </p>\n<p>상세 사양은 아래의 링크에서 확인이 가능하다.</p>\n<p><a href=\"https://members.pcisig.com/wg/PCI-SIG/document/20711\">CopprLink Internal Cable</a> / <a href=\"https://members.pcisig.com/wg/PCI-SIG/document/20866\">CopprLink External Cable</a></p>\n<p>PCI-SIG의 회원일 경우에는 각 $50, 비회원의 경우에는 $4,500의 금액을 지불하고 열람할 수 있다.</p>\n<p>SNIA에서 관리하는 SFF 커넥터들에 대해서는 언젠가 시간이 될 때 다루어보도록 하자.</p>\n<hr>\n<h3>참고자료</h3>\n<ul>\n<li><a href=\"https://pcisig.com/blog/pcie®-cabling-–-journey-copprlink™\">PCI-SIG 블로그</a></li>\n<li><a href=\"https://www.anandtech.com/show/21379/pcisig-completes-copprlink-cabling-standard-pcie-50-60-get-wired\">Anandtech에서 상세한 자료를 읽기 귀찮은 사람들을 위해 잘 정리해주었다.</a></li>\n</ul>\n<p> </p>",
            "image": "https://freshflash4096.github.io/media/posts/19/PCI-SIG-Cabling-Initiatives.png",
            "author": {
                "name": "Huie"
            },
            "tags": [
                   "Archive"
            ],
            "date_published": "2024-05-02T22:49:01+09:00",
            "date_modified": "2025-06-20T22:54:30+09:00"
        },
        {
            "id": "https://freshflash4096.github.io/purchasing-pm883-8tb/",
            "url": "https://freshflash4096.github.io/purchasing-pm883-8tb/",
            "title": "PM883 7.68TB *2 구매",
            "summary": "Samsung PM883 7.68TB NVMe와 SATA 중에 어떤 것을 메인으로 사용할지 고민하다가 SATA로&hellip;",
            "content_html": "<div class=\"gallery-wrapper\"><div class=\"gallery\"  data-is-empty=\"false\" data-translation=\"이미지 추가\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/15/gallery/PM883.jpg\" data-size=\"4128x3096\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/15/gallery/PM883-thumbnail.jpg\" alt=\"Samsung PM883 7.68TB\" width=\"768\" height=\"576\"></a>\n<figcaption>Samsung PM883 7.68TB</figcaption>\n</figure>\n</div></div>\n<p>NVMe와 SATA 중에 어떤 것을 메인으로 사용할지 고민하다가 SATA로 결정하고 2024년 3월 30일에 구매했다. 물론 NVMe가 처리량도, 대기시간도 우수하고 가격마저도 비슷하지만... IDLE 전력과 PCIe 레인 확보의 어려움 때문에 SATA로 결정했다. </p>\n<p>추후 랩을 축소할 때도 더 좋을 것이라고 판단했고.</p>\n<p>7.68TB가 SATA대역폭으로 감당이 가능한가 하면 약간 답답하다. SATA에서는 3.84TB가 Best point, 7.68TB는 한계치라고 생각한다. 이 위로 용량을 올릴거면 NVMe로 가는게 좋지 않을까?</p>\n<p><a href=\"https://kb.open-e.com/samsung-pm863pm863a-and-pm883-data-miscompare-issue_3321.html\">펌웨어 이슈</a>가 있는 것은 알고 있지만, 그만큼 커뮤니티의 자료도 꽤 많아 추후 펌웨어 업데이트를 하고 서버가 완성될 때 투입할 예정이다. 이에 대해서는 나중에 별도로 포스팅할 예정이다.</p>\n<hr>\n<figure class=\"post__image\" ><figure class=\"post__image post__image--center\"><img loading=\"lazy\"  src=\"https://freshflash4096.github.io/media/posts/15/seukeurinsyas-2024-04-28-ojeon-12.50.31.png\" alt=\"\" width=\"2386\" height=\"474\"></figure>\n<figcaption ><a href=\"https://www.synology.com/ko-kr/compatibility?search_by=drives&amp;model=DS620slim&amp;category=hdds_no_ssd_trim&amp;display_brand=other&amp;filter_brand=Samsung&amp;filter_class=Enterprise\">Synology HDD/SSD QVL</a></figcaption>\n</figure>\n<p>DS620slim 기준으로 PM883을 호환성 목록에 넣어놨지만 <span style=\"text-decoration: underline;\"><em>\"This drive doesn't support TRIM.\"</em></span><em> </em>이라는 문구를 덧붙혀놨다.</p>\n<div class=\"gallery-wrapper\"><div class=\"gallery\"  data-is-empty=\"false\" data-translation=\"이미지 추가\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/15/gallery/seukeurinsyas-2024-03-30-ohu-11.43.04.png\" data-size=\"1382x946\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/15/gallery/seukeurinsyas-2024-03-30-ohu-11.43.04-thumbnail.png\" alt=\"SSD TRIM option with Another SSD\" width=\"768\" height=\"526\"></a>\n<figcaption>SSD TRIM option with Another SSD</figcaption>\n</figure>\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/15/gallery/seukeurinsyas-2024-03-30-ohu-11.43.38.png\" data-size=\"1382x946\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/15/gallery/seukeurinsyas-2024-03-30-ohu-11.43.38-thumbnail.png\" alt=\"SSD TRIM option with PM883\" width=\"768\" height=\"526\"></a>\n<figcaption>SSD TRIM option with PM883</figcaption>\n</figure>\n</div></div>\n<p>실제로 DSM상에서 TRIM을 설정하기 위해 저장소 관리자를 들어가면 위와 같이 TRIM을 설정할 수 있는 옵션이 아예 존재하지 않는다. DSM에서 쓸 생각은 없기에 더 탐구해보진 않을 것이다.</p>",
            "image": "https://freshflash4096.github.io/media/posts/15/Samsung-PM883-7.68TB.jpeg",
            "author": {
                "name": "Huie"
            },
            "tags": [
                   "Homelab",
                   "Archive"
            ],
            "date_published": "2024-04-28T00:33:27+09:00",
            "date_modified": "2025-06-20T22:53:36+09:00"
        },
        {
            "id": "https://freshflash4096.github.io/purchasing-eaton-5p650ir/",
            "url": "https://freshflash4096.github.io/purchasing-eaton-5p650ir/",
            "title": "5P650IR 구매와 팬교체",
            "summary": "5P650IR on Desk 5P650IR teardown 2024년 1월 17일, 10Kg이 넘는 택배가 도착했다.",
            "content_html": "<div class=\"gallery-wrapper\"><div class=\"gallery\"  data-is-empty=\"false\" data-translation=\"이미지 추가\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/14/gallery/5P650IR.png\" data-size=\"3428x1806\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/14/gallery/5P650IR-thumbnail.png\" alt=\"5P650IR on Desk\" width=\"768\" height=\"405\"></a>\n<figcaption>5P650IR on Desk</figcaption>\n</figure>\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/14/gallery/5P650IR-Fanswap.png\" data-size=\"1707x1280\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/14/gallery/5P650IR-Fanswap-thumbnail.png\" alt=\"5P650IR teardown\" width=\"768\" height=\"576\"></a>\n<figcaption>5P650IR teardown</figcaption>\n</figure>\n</div></div>\n<p>2024년 1월 17일, 10Kg이 넘는 택배가 도착했다. UPS 너무 무겁다...</p>\n<p>국내에서 구할 수 있는 5P850IR이 아닌 더 저용량의 버전을 선택한 이유로 첫 번째는 깊이가 짧다는 것, 두 번째는 싸기 때문이다.</p>\n<p>포트 수는 아쉽기에 추후 PDU등으로 확장할 생각인데... Eaton의 케이블이 아니면 후면에 장착하는 케이블 정리 부품을 사용하지 못한다. 고민거리가 또 늘었다.</p>\n<p>1U 장비라 예상은 했는데 충전 중의 소음이 상당하고 95%가 충전되면 팬속이 급감한다. 못 쓸 정도는 아니지만 귀에 상당히 거슬리는 소리라 팬 교체를 결심했다.</p>\n<p> </p>\n<figure class=\"post__image\"><img loading=\"lazy\"  src=\"https://noctua.at/pub/media/catalog/product/cache/74c1057f7991b4edb2bc7bdaa94de933/n/o/noctua_nf_a4x20_flx_5_1.jpg\" alt=\"NF-A4x20 FLX\"><figcaption>NF-A4x20 FLX</figcaption></figure>\n<p><a href=\"https://www.reddit.com/r/homelab/comments/cz5846/report_quiet_eaton_ups_by_replacing_stock_fan/\">레딧의 글</a>을 참고했고, 교체한 팬은 Noctua NF-A4x20 FLX다. 국내에는 정식수입이 되지않아 찾아보는 도중에 중고나라에서 팔고 있는 것을 보고 주워왔다. </p>\n<p>기본 구성품에 포함된 3M 심선접속자를 이용했는데 여러번의 실패로 인해 그냥 50개짜리를 사서 기어코 성공해냈다. 그렇게 많은 힘이 필요할줄은 몰랐지...</p>\n<p>저항을 연결해서 사용 중인데 조용해서 맘에든다. 근데 따끈따끈함이 느껴져서... 상시 에어컨을 켤 수 있는 환경도 아니고 바로 위에 라우터의 열기에도 영향을 받을테니 이후 서버를 완성할 때, UPS의 전원을 끄고 저항은 빼기로 한다.</p>\n<p>UPS에서 볼 수 있는 전력 사용량은 당연하지만 벽전력은 아니고 4W 단위로 측정되는 것을 보니 아마 1% 단위로 측정하는 것 같다.</p>\n<p>물려놓은게 얼마 없어서 그런지 효율이 꽤 낮아보여 NAS와 네트워크 기기들을 제외하고도 모니터, 데스크탑, 충전기를 물려놨다.</p>\n<p>이 글을 작성하는 지금, 로드되는 전력은 61W고 효율은 ~81%로 잡히는 중이다.</p>\n<p>여유롭구만</p>\n<p> </p>\n<p><!-- notionvc: 625e3410-be5b-4030-adde-4bfd56125804 --></p>",
            "image": "https://freshflash4096.github.io/media/posts/14/5P-RACK_R.jpeg",
            "author": {
                "name": "Huie"
            },
            "tags": [
                   "Homelab",
                   "Archive"
            ],
            "date_published": "2024-04-28T00:05:43+09:00",
            "date_modified": "2025-06-20T22:53:06+09:00"
        },
        {
            "id": "https://freshflash4096.github.io/purchasing-udm-pro/",
            "url": "https://freshflash4096.github.io/purchasing-udm-pro/",
            "title": "Dream Machine Pro 구매와 네트워크 구성 고민",
            "summary": "Homelab on Desk 2023년 12월 8일, 블프 할인가로 구매한 UDM-Pro가 도착했다. 참고로&hellip;",
            "content_html": "<div class=\"gallery-wrapper\"><div class=\"gallery\"  data-is-empty=\"false\" data-translation=\"이미지 추가\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/13/gallery/UDM-Pro.png\" data-size=\"1506x1130\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/13/gallery/UDM-Pro-thumbnail.png\" alt=\"Homelab on Desk\" width=\"768\" height=\"576\"></a>\n<figcaption>Homelab on Desk</figcaption>\n</figure>\n</div></div>\n<p>2023년 12월 8일, 블프 할인가로 구매한 UDM-Pro가 도착했다. </p>\n<p>참고로 이날 오전에 UDM-Pro-Max의 정보가 유출되었고, 이 글을 작성하는 2024년 4월 27일 기준으로는 이미 <a href=\"https://store.ui.com/us/en/collections/unifi-dream-machine/products/udm-pro-max\">발매된 상태</a>이다. </p>\n<p>내 기준으로는 UDM-Pro면 충분해서 아마 업그레이드 할 일은 없을 것이다.</p>\n<p> </p>\n<div class=\"gallery-wrapper\"><div class=\"gallery\"  data-is-empty=\"false\" data-translation=\"이미지 추가\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/13/gallery/seukeurinsyas-Dec-8-Screenshot.png\" data-size=\"3840x2160\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/13/gallery/seukeurinsyas-Dec-8-Screenshot-thumbnail.png\" alt=\"Topology with USG\" width=\"768\" height=\"432\"></a>\n<figcaption>Topology with USG</figcaption>\n</figure>\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/13/gallery/seukeurinsyas-2024-04-27-ohu-11.44.22.png\" data-size=\"4608x2592\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/13/gallery/seukeurinsyas-2024-04-27-ohu-11.44.22-thumbnail.png\" alt=\"Topology with UDM-Pro\" width=\"768\" height=\"432\"></a>\n<figcaption>Topology with UDM-Pro</figcaption>\n</figure>\n</div></div>\n<p>좌측은 USG에서의 마지막 토폴로지다. 기존 설정에서 마이그레이션을 진행하지 않고 처음부터 새로 세팅해서 대략 5시간이 걸렸다. </p>\n<p>우측은 UDM-Pro를 세팅 완료한 글 작성 시점의 토폴로지다.</p>\n<p>추후 랙마운트 스위치를 구매해 US-8-60W를 대체할 생각인데, 아마 <a href=\"https://store.ui.com/us/en/collections/unifi-switching-standard-power-over-ethernet/products/usw-16-poe\">USW-16-PoE</a>를 구매하게 될 것 같고, 단자함의 스위치는 USW-Flex-Mini에서 <a href=\"https://store.ui.com/us/en/collections/unifi-switching-utility-poe/products/usw-ultra\">USW-Ultra</a>로 업그레이드할 계획이다.</p>\n<hr>\n<figure class=\"post__image\" ><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/13/seukeurinsyas-2024-04-27-ohu-11.49.18.png\" alt=\"\" width=\"1672\" height=\"546\">\n<figcaption ><a href=\"https://help.ui.com/hc/en-us/articles/360042281174-UniFi-Switch-Layer-3-Routing\">UniFi Switch - Layer 3 Routing</a></figcaption>\n</figure>\n<p>고민이 있다면 UniFi Switch 라인업 중 Pro가 아닌 Standard 라인업은 VLAN Routing을 지원하지 않기에 UDM-Pro까지 거슬러 올라가야 하는데... Standard 라인업은 10GbE SFP+가 아닌 1GbE SFP 포트라는 것이다.</p>\n<p>현재 상황으로는 목적별로 Management, Server, Client, IoT, Guest, VPN 등으로 VLAN을 나누어 놓았는데 다른 건 크게 상관없지만 Server와 Client간의 병목이 예상된다.</p>\n<p>물론 <a href=\"https://store.ui.com/us/en/collections/unifi-switching-standard-power-over-ethernet/products/usw-16-poe\">USW-16-PoE</a>가 아닌 <a href=\"https://store.ui.com/us/en/collections/unifi-switching-pro-power-over-ethernet/products/usw-pro-24-poe\">USW-Pro-24-PoE</a>를 구매한다는 방법도 있지만 가격과 소음이 문제다.</p>\n<p><a href=\"https://store.ui.com/us/en/collections/unifi-switching-pro-ethernet/products/usw-pro-24\">USW-Pro-24</a>와 PoE용 스위치를 별도로 구분할까도 생각해봤는데, 쓸데없는 지출같아서 기각했다.</p>\n<hr>\n<p>그 다음 고민은 IoT VLAN의 세팅이다. 위의 UDM-Pro와 함께한 토폴로지를 유심히 보면 알겠지만 공기청정기가 인터넷을 요구해서 IoT가 아닌 Guest VLAN으로 할당해놓았다.</p>\n<p>내가 기존에 설정한 IoT VLAN은 WAN과 LAN에 접근이 불가능하기 때문에 인터넷 연결이 안된다고 울부짖어대서 여차여차 Guest VLAN에 연결해놓았는데, IoT VLAN에 WAN의 접근을 허용해야할지...</p>\n<p>아니면 IoT VLAN의 WAN은 VPN을 타고 나가게 한다는 방법도 있기는 한데... 개인적으로 IoT는 로컬로만 동작할 수 있게 구성하는게 좋은 것 같다.</p>",
            "image": "https://freshflash4096.github.io/media/posts/13/udm-pro-2.png",
            "author": {
                "name": "Huie"
            },
            "tags": [
                   "Homelab",
                   "Archive"
            ],
            "date_published": "2024-04-27T23:09:22+09:00",
            "date_modified": "2025-06-20T22:52:48+09:00"
        },
        {
            "id": "https://freshflash4096.github.io/record-2023/",
            "url": "https://freshflash4096.github.io/record-2023/",
            "title": "2023년을 기록하며",
            "summary": "S4500 3.84TB *3 구매 [2023.03.22.] Intel SSD DC S4500 3.84TB 이 시점에서&hellip;",
            "content_html": "<h3>S4500 3.84TB *3 구매 [2023.03.22.]</h3>\n<div class=\"gallery-wrapper\"><div class=\"gallery\"  data-is-empty=\"false\" data-translation=\"이미지 추가\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/12/gallery/S4500.jpg\" data-size=\"4128x3096\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/12/gallery/S4500-thumbnail.jpg\" alt=\"Intel SSD DC S4500 3.84TB\" width=\"768\" height=\"576\"></a>\n<figcaption>Intel SSD DC S4500 3.84TB</figcaption>\n</figure>\n</div></div>\n<p>이 시점에서 계속해서 생각만하던 All Flash 구성을 달성했다. DS620slim의 HW 한계상 속도를 뽑거나 하긴 힘들지만 소음이 줄어든다는 것만으로도 큰 이점을 체감했다.</p>\n<hr>\n<h3>P4800X 750GB 구매 [2023.03.31.]</h3>\n<div class=\"gallery-wrapper\"><div class=\"gallery\"  data-is-empty=\"false\" data-translation=\"이미지 추가\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/12/gallery/P4800X.jpg\" data-size=\"4128x2580\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/12/gallery/P4800X-thumbnail.jpg\" alt=\"Intel Optane SSD DC P4800X 750GB\" width=\"768\" height=\"480\"></a>\n<figcaption>Intel Optane SSD DC P4800X 750GB</figcaption>\n</figure>\n</div></div>\n<p>신기하게도 이 날은 900P를 두 개 구입한지 딱 1년이 되는 날이다. 목적은 ZFS의 SLOG에서 그냥 부팅용 드라이브로 바뀌었다.</p>\n<p>P5810X 400GB 정도로 끝장을 보고 싶긴했는데 아직 내겐 너무 비싸다.</p>\n<p>나중에 그것도 구매하는걸로 ㅎㅎ;</p>\n<hr>\n<h3>U2723QE 구매 [2023.04.11.]</h3>\n<div class=\"gallery-wrapper\"><div class=\"gallery\"  data-is-empty=\"false\" data-translation=\"이미지 추가\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/12/gallery/U2723QE.png\" data-size=\"2276x1280\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/12/gallery/U2723QE-thumbnail.png\" alt=\"Dell Ultrasharp U2723QE\" width=\"768\" height=\"432\"></a>\n<figcaption>Dell Ultrasharp U2723QE</figcaption>\n</figure>\n</div></div>\n<p>내가 원하는 모든 것을 만족시키는 것은 아니지만 최선의 모니터를 구매했다. 기존의 P2419HC도 잘 쓰고 있긴했지만 새로 산 이유는 동생에게 P2419HC를 준다는 변명과 할인 때문이었다.</p>\n<p>24\"에서 27\"로 넘어온 것은 정말 만족스럽다. 가끔 예전에 사용하던 듀얼 모니터가 그립긴한데 U2723QE로 듀얼 모니터를 세팅하는 건 지갑이 너무 아프다. 태블릿으로 대체하려고 고민 중이긴한데 문제점이 꽤 많다.</p>\n<hr>\n<h3>Mac mini 구매 [2023.06.05]</h3>\n<div class=\"gallery-wrapper\"><div class=\"gallery\"  data-is-empty=\"false\" data-translation=\"이미지 추가\" data-columns=\"3\">\n<figure class=\"gallery__item\"><a href=\"https://freshflash4096.github.io/media/posts/12/gallery/Mac-mini.png\" data-size=\"2276x1280\"><img loading=\"lazy\" src=\"https://freshflash4096.github.io/media/posts/12/gallery/Mac-mini-thumbnail.png\" alt=\"Apple Mac mini\" width=\"768\" height=\"432\"></a>\n<figcaption>Apple Mac mini</figcaption>\n</figure>\n</div></div>\n<pre class=\"language-ini\"><code>CPU: Apple M2 (8C)\nGPU: Apple M2 (10C)\nRAM: LPDDR5 16GB\nSSD: Apple SSD AP0256Z</code></pre>\n<p>저전력이라는 점에 힘입어 로컬 AI와 컨테이너 등을 돌릴 목적으로 구매했고 생각보다 상당히 저전력이라 그냥 데스크탑으로 쓰기로했다. 성능, 특히 GPU가 꽤 부족하긴한데... 추후 M3 Pro버전 등을 고민하면서 넘겼다.</p>\n<p>메모리는 쓰다보면 부족해져서 재부팅을 해주고 신경을 약간 써줘야 하는 것은 흠인 것 같다. 나만 그런건가? </p>\n<hr>\n<p>맥미니를 마지막으로 한동안 구매를 중단하다가 연말에 가서 UDM-Pro를 기반으로 네트워크를 재구성했는데, 이는 별도의 포스트로 남긴다.</p>",
            "author": {
                "name": "Huie"
            },
            "tags": [
                   "Homelab",
                   "Archive"
            ],
            "date_published": "2024-04-27T20:40:04+09:00",
            "date_modified": "2025-06-20T22:52:07+09:00"
        }
    ]
}
